{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0307b4d-e209-45ad-b93b-0aa5eb1b7688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: great_expectations in /opt/conda/lib/python3.11/site-packages (1.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: pandasql in /opt/conda/lib/python3.11/site-packages (0.7.3)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: altair<5.0.0,>=4.2.1 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.2.2)\n",
      "Requirement already satisfied: cryptography>=3.2 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (41.0.4)\n",
      "Requirement already satisfied: jinja2>=2.10 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.19.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.7.1 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.26.1)\n",
      "Requirement already satisfied: mistune>=0.8.4 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from great_expectations) (23.2)\n",
      "Requirement already satisfied: posthog<4,>3 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.13.0)\n",
      "Requirement already satisfied: pydantic>=1.10.7 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (2.10.6)\n",
      "Requirement already satisfied: pyparsing>=2.4 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (2.31.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.16 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (0.17.39)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.59.0 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.12.2)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (5.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.11/site-packages (from pandasql) (2.0.22)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.4)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=3.2->great_expectations) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2>=2.10->great_expectations) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from posthog<4,>3->great_expectations) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.11/site-packages (from posthog<4,>3->great_expectations) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from posthog<4,>3->great_expectations) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2023.7.22)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.11/site-packages (from ruamel.yaml>=0.16->great_expectations) (0.2.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy->pandasql) (3.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849767 sha256=9e2a3cd9bae7c0fa7faffe668cf8d538be658e6e69d96e13db7ee15865d4f324\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/8d/28/22/5dbae8a8714ef046cebd320d0ef7c92f5383903cf854c15c0c\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install great_expectations pandas pandasql pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "982d3986-a04a-4d85-ae9e-c93e81135c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import pandasql as psql\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f1a0363-8e65-4305-829f-39ab7c85af76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": false,\n",
       "  \"expectation_config\": {\n",
       "    \"type\": \"unexpected_rows_expectation\",\n",
       "    \"kwargs\": {\n",
       "      \"unexpected_rows_query\": \"\\n    SELECT\\n        *\\n    FROM\\n        df_transfers\\n    WHERE\\n        amount < 200\",\n",
       "      \"batch_id\": \"example-transfers\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  \"result\": {},\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"exception_traceback\": \"Traceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/expectations/registry.py\\\", line 315, in get_metric_provider\\n    return metric_definition[\\\"providers\\\"][type(execution_engine).__name__]\\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nKeyError: 'PandasExecutionEngine'\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/validator/validator.py\\\", line 714, in _generate_metric_dependency_subgraphs_for_each_expectation_configuration\\n    graph=self._metrics_calculator.build_metric_dependency_graph(\\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/validator/metrics_calculator.py\\\", line 200, in build_metric_dependency_graph\\n    graph.build_metric_dependency_graph(\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/validator/validation_graph.py\\\", line 134, in build_metric_dependency_graph\\n    ) = self.set_metric_configuration_default_kwargs_if_absent(\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/validator/validation_graph.py\\\", line 178, in set_metric_configuration_default_kwargs_if_absent\\n    metric_impl_klass, metric_provider = get_metric_provider(\\n                                         ^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/expectations/registry.py\\\", line 317, in get_metric_provider\\n    raise gx_exceptions.MetricProviderError(  # noqa: TRY003 # FIXME CoP\\ngreat_expectations.exceptions.exceptions.MetricProviderError: No provider found for unexpected_rows_query.table using PandasExecutionEngine\\n\",\n",
       "    \"exception_message\": \"No provider found for unexpected_rows_query.table using PandasExecutionEngine\",\n",
       "    \"raised_exception\": true\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUERY NON FUNZIONANTE SU DATAFRAME\n",
    "# Simulate the DataFrame (this would represent your data asset in the example)\n",
    "data_transfers = {\n",
    "    'transfer_balance_id': [1, 2, 3, 4],\n",
    "    'amount': [100, 200, 150, 500],\n",
    "    'transfer_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']\n",
    "}\n",
    "\n",
    "data_transfer_balance = {\n",
    "    'transfer_balance_id': [1, 2, 3, 4],\n",
    "    'total_amount': [100, 200, 150, 500]\n",
    "}\n",
    "\n",
    "df_transfers = pd.DataFrame(data_transfers)\n",
    "df_transfer_balance = pd.DataFrame(data_transfer_balance)\n",
    "\n",
    "# Create Batch from DataFrame (this simulates your batch in the context)\n",
    "context = gx.get_context()\n",
    "\n",
    "data_source = context.data_sources.add_pandas(\"example\")\n",
    "transfers_data_asset = data_source.add_dataframe_asset(name=\"transfers\")\n",
    "transfer_balance_data_asset = data_source.add_dataframe_asset(name=\"transfer_balance\")\n",
    "\n",
    "batch_transfers = transfers_data_asset.add_batch_definition_whole_dataframe(\"transfers\").get_batch(batch_parameters={\"dataframe\": df_transfers})\n",
    "batch_transfer_balance = transfer_balance_data_asset.add_batch_definition_whole_dataframe(\"transfer_balance\").get_batch(batch_parameters={\"dataframe\": df_transfer_balance})\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        {batch}\n",
    "    WHERE\n",
    "        amount < 200\n",
    "    \"\"\"\n",
    "\n",
    "query_result = psql.sqldf(query, locals())\n",
    "\n",
    "query_expectation = gx.expectations.UnexpectedRowsExpectation(\n",
    "    unexpected_rows_query=query\n",
    ")\n",
    "\n",
    "batch_transfers.validate(query_expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a33a674-e215-49bc-9522-1376a2c8f6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgx\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Inizializza una sessione Spark\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataFrames Example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Crea i dati\u001b[39;00m\n\u001b[1;32m      9\u001b[0m data_transfers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     11\u001b[0m     (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-01-02\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     12\u001b[0m     (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m150\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-01-03\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     13\u001b[0m     (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-01-04\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import great_expectations as gx\n",
    "\n",
    "# Inizializza una sessione Spark\n",
    "spark = SparkSession.builder.appName(\"DataFrames Example\").getOrCreate()\n",
    "\n",
    "# Crea i dati\n",
    "data_transfers = [\n",
    "    (1, 100, '2023-01-01'),\n",
    "    (2, 200, '2023-01-02'),\n",
    "    (3, 150, '2023-01-03'),\n",
    "    (4, 500, '2023-01-04')\n",
    "]\n",
    "\n",
    "data_transfer_balance = [\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 150),\n",
    "    (4, 500)\n",
    "]\n",
    "\n",
    "# Crea DataFrame in Spark\n",
    "df_transfers = spark.createDataFrame(data_transfers, [\"transfer_balance_id\", \"amount\", \"transfer_date\"])\n",
    "df_transfer_balance = spark.createDataFrame(data_transfer_balance, [\"transfer_balance_id\", \"total_amount\"])\n",
    "\n",
    "# Crea una sessione di Great Expectations\n",
    "context = gx.get_context()\n",
    "\n",
    "# Crea una fonte di dati per Great Expectations\n",
    "data_source = context.data_sources.add_pandas(\"example\")\n",
    "transfers_data_asset = data_source.add_dataframe_asset(name=\"transfers\")\n",
    "transfer_balance_data_asset = data_source.add_dataframe_asset(name=\"transfer_balance\")\n",
    "\n",
    "# Crea batch per i dati\n",
    "batch_transfers = transfers_data_asset.add_batch_definition_whole_dataframe(\"transfers\").get_batch(batch_parameters={\"dataframe\": df_transfers})\n",
    "batch_transfer_balance = transfer_balance_data_asset.add_batch_definition_whole_dataframe(\"transfer_balance\").get_batch(batch_parameters={\"dataframe\": df_transfer_balance})\n",
    "\n",
    "# Esegui una join (merge) dei due DataFrame\n",
    "merged_df = df_transfers.join(df_transfer_balance, on=\"transfer_balance_id\", how=\"left\")\n",
    "\n",
    "# Crea una vista temporanea per eseguire query SQL su Spark\n",
    "merged_df.createOrReplaceTempView(\"transfers_table\")\n",
    "\n",
    "# Definisci la query SQL per ottenere le righe con 'amount' < 200\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM transfers_table\n",
    "    WHERE amount < 200\n",
    "\"\"\"\n",
    "\n",
    "# Esegui la query SQL su Spark\n",
    "query_result = spark.sql(query)\n",
    "\n",
    "# Mostra il risultato della query\n",
    "query_result.show()\n",
    "\n",
    "# Definisci l'aspettativa di Great Expectations per convalidare le righe che non corrispondono\n",
    "query_expectation = gx.expectations.UnexpectedRowsExpectation(\n",
    "    unexpected_rows_query=query\n",
    ")\n",
    "\n",
    "# Verifica il batch per la validazione\n",
    "batch_transfers.validate(query_expectation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c10b53-42dd-4593-8d0c-f70fdfba59ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simula i tuoi DataFrame (questi sono i dati)\n",
    "data_transfers = {\n",
    "    'transfer_balance_id': [1, 2, 3, 4],\n",
    "    'amount': [100, 200, 150, 500],\n",
    "    'transfer_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']\n",
    "}\n",
    "\n",
    "data_transfer_balance = {\n",
    "    'transfer_balance_id': [1, 2, 3],#, 4],\n",
    "    'total_amount': [100, 200, 150, 500]\n",
    "}\n",
    "\n",
    "df_transfers = pd.DataFrame(data_transfers)\n",
    "df_transfer_balance = pd.DataFrame(data_transfer_balance)\n",
    "\n",
    "# Esegui la merge dei DataFrame\n",
    "merged_df = pd.merge(df_transfers, df_transfer_balance, on='transfer_balance_id', how='left')\n",
    "\n",
    "print(merged_df)\n",
    "\n",
    "# Crea il contesto di Great Expectations\n",
    "context = gx.get_context()\n",
    "\n",
    "data_source = context.data_sources.add_pandas(\"example\")\n",
    "merge_data_asset = data_source.add_dataframe_asset(name=\"merge\")\n",
    "merge_batch = merge_data_asset.add_batch_definition_whole_dataframe(\"merge\").get_batch(batch_parameters={\"dataframe\": merged_df})\n",
    "\n",
    "\n",
    "# Verifica se i valori nelle colonne 'amount' e 'total_amount' sono uguali\n",
    "expectation = gx.expectations.core.ExpectColumnPairValuesToBeEqual(\n",
    "    column_A=\"amount\",\n",
    "    column_B=\"total_amount\"\n",
    ")\n",
    "validation_result = merge_batch.validate(expectation)\n",
    "print(validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be651ed4-db59-48c4-ad83-de1916a8a2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
