{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0307b4d-e209-45ad-b93b-0aa5eb1b7688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting great_expectations\n",
      "  Using cached great_expectations-1.3.7-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting pandasql\n",
      "  Using cached pandasql-0.7.3-py3-none-any.whl\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.4-py2.py3-none-any.whl\n",
      "Collecting altair<5.0.0,>=4.2.1 (from great_expectations)\n",
      "  Using cached altair-4.2.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cryptography>=3.2 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (41.0.4)\n",
      "Requirement already satisfied: jinja2>=3 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.19.1)\n",
      "Collecting marshmallow<4.0.0,>=3.7.1 (from great_expectations)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: mistune>=0.8.4 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from great_expectations) (23.2)\n",
      "Collecting posthog<4,>3 (from great_expectations)\n",
      "  Downloading posthog-3.15.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pydantic>=1.10.7 (from great_expectations)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pyparsing>=2.4 (from great_expectations)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (2.31.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.16 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (0.17.39)\n",
      "Collecting scipy>=1.6.0 (from great_expectations)\n",
      "  Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: tqdm>=4.59.0 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.8.0)\n",
      "Collecting tzlocal>=1.2 (from great_expectations)\n",
      "  Using cached tzlocal-5.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting numpy>=1.22.4 (from great_expectations)\n",
      "  Using cached numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting numpy>=1.22.4 (from great_expectations)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.11/site-packages (from pandasql) (2.0.22)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.4)\n",
      "Collecting toolz (from altair<5.0.0,>=4.2.1->great_expectations)\n",
      "  Using cached toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=3.2->great_expectations) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2>=3->great_expectations) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from posthog<4,>3->great_expectations) (1.16.0)\n",
      "Collecting monotonic>=1.5 (from posthog<4,>3->great_expectations)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<4,>3->great_expectations)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=1.10.7->great_expectations)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=1.10.7->great_expectations)\n",
      "  Using cached pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.1.0 (from great_expectations)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2023.7.22)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.11/site-packages (from ruamel.yaml>=0.16->great_expectations) (0.2.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy->pandasql) (3.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\n",
      "Using cached great_expectations-1.3.7-py3-none-any.whl (5.0 MB)\n",
      "Using cached pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Using cached altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Downloading posthog-3.15.1-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m497.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached tzlocal-5.3-py3-none-any.whl (17 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Installing collected packages: py4j, monotonic, tzlocal, tzdata, typing-extensions, toolz, pyspark, pyparsing, numpy, marshmallow, backoff, annotated-types, scipy, pydantic-core, posthog, pandas, pydantic, pandasql, altair, great_expectations\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed altair-4.2.2 annotated-types-0.7.0 backoff-2.2.1 great_expectations-1.3.7 marshmallow-3.26.1 monotonic-1.6 numpy-1.26.4 pandas-2.1.4 pandasql-0.7.3 posthog-3.15.1 py4j-0.10.9.7 pydantic-2.10.6 pydantic-core-2.27.2 pyparsing-3.2.1 pyspark-3.5.4 scipy-1.15.2 toolz-1.0.0 typing-extensions-4.12.2 tzdata-2025.1 tzlocal-5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install great_expectations pandas pandasql pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982d3986-a04a-4d85-ae9e-c93e81135c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import pandasql as psql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1a0363-8e65-4305-829f-39ab7c85af76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": false,\n",
       "  \"expectation_config\": {\n",
       "    \"type\": \"unexpected_rows_expectation\",\n",
       "    \"kwargs\": {\n",
       "      \"unexpected_rows_query\": \"\\n    SELECT\\n        *\\n    FROM\\n        {batch}\\n    WHERE\\n        amount < 200\",\n",
       "      \"batch_id\": \"example-transfers\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  \"result\": {},\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"exception_traceback\": \"Traceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/expectations/registry.py\\\", line 315, in get_metric_provider\\n    return metric_definition[\\\"providers\\\"][type(execution_engine).__name__]\\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nKeyError: 'PandasExecutionEngine'\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/validator/validator.py\\\", line 714, in _generate_metric_dependency_subgraphs_for_each_expectation_configuration\\n    graph=self._metrics_calculator.build_metric_dependency_graph(\\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/validator/metrics_calculator.py\\\", line 200, in build_metric_dependency_graph\\n    graph.build_metric_dependency_graph(\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/validator/validation_graph.py\\\", line 134, in build_metric_dependency_graph\\n    ) = self.set_metric_configuration_default_kwargs_if_absent(\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/validator/validation_graph.py\\\", line 178, in set_metric_configuration_default_kwargs_if_absent\\n    metric_impl_klass, metric_provider = get_metric_provider(\\n                                         ^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/conda/lib/python3.11/site-packages/great_expectations/expectations/registry.py\\\", line 317, in get_metric_provider\\n    raise gx_exceptions.MetricProviderError(  # noqa: TRY003 # FIXME CoP\\ngreat_expectations.exceptions.exceptions.MetricProviderError: No provider found for unexpected_rows_query.table using PandasExecutionEngine\\n\",\n",
       "    \"exception_message\": \"No provider found for unexpected_rows_query.table using PandasExecutionEngine\",\n",
       "    \"raised_exception\": true\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUERY SQL NON FUNZIONANTE SU DATAFRAME PANDAS\n",
    "data_transfers = {\n",
    "    'transfer_balance_id': [1, 2, 3, 4],\n",
    "    'amount': [100, 200, 150, 500],\n",
    "    'transfer_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']\n",
    "}\n",
    "\n",
    "data_transfer_balance = {\n",
    "    'transfer_balance_id': [1, 2, 3, 4],\n",
    "    'total_amount': [100, 200, 150, 500]\n",
    "}\n",
    "\n",
    "df_transfers = pd.DataFrame(data_transfers)\n",
    "df_transfer_balance = pd.DataFrame(data_transfer_balance)\n",
    "\n",
    "context = gx.get_context()\n",
    "\n",
    "data_source = context.data_sources.add_pandas(\"example\")\n",
    "transfers_data_asset = data_source.add_dataframe_asset(name=\"transfers\")\n",
    "transfer_balance_data_asset = data_source.add_dataframe_asset(name=\"transfer_balance\")\n",
    "\n",
    "batch_transfers = transfers_data_asset.add_batch_definition_whole_dataframe(\"transfers\").get_batch(batch_parameters={\"dataframe\": df_transfers})\n",
    "batch_transfer_balance = transfer_balance_data_asset.add_batch_definition_whole_dataframe(\"transfer_balance\").get_batch(batch_parameters={\"dataframe\": df_transfer_balance})\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        {batch}\n",
    "    WHERE\n",
    "        amount < 200\n",
    "    \"\"\"\n",
    "\n",
    "query_expectation = gx.expectations.UnexpectedRowsExpectation(\n",
    "    unexpected_rows_query=query\n",
    ")\n",
    "\n",
    "batch_transfers.validate(query_expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f60a424-c24f-4d3b-8d78-a05d3efde009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c10b53-42dd-4593-8d0c-f70fdfba59ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 9/9 [00:00<00:00, 258.53it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": true,\n",
       "  \"expectation_config\": {\n",
       "    \"type\": \"expect_column_pair_values_to_be_equal\",\n",
       "    \"kwargs\": {\n",
       "      \"batch_id\": \"example-merge\",\n",
       "      \"column_A\": \"amount\",\n",
       "      \"column_B\": \"total_amount\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  \"result\": {\n",
       "    \"element_count\": 4,\n",
       "    \"unexpected_count\": 0,\n",
       "    \"unexpected_percent\": 0.0,\n",
       "    \"partial_unexpected_list\": [],\n",
       "    \"missing_count\": 0,\n",
       "    \"missing_percent\": 0.0,\n",
       "    \"unexpected_percent_total\": 0.0,\n",
       "    \"unexpected_percent_nonmissing\": 0.0,\n",
       "    \"partial_unexpected_counts\": [],\n",
       "    \"partial_unexpected_index_list\": []\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FUNZIONANTE DATAFRAME PANDAS + JOIN DATAFRAME (SENZA QUERY SQL)\n",
    "data_transfers = {\n",
    "    'transfer_balance_id': [1, 2, 3, 4],\n",
    "    'amount': [100, 200, 150, 500],\n",
    "    'transfer_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']\n",
    "}\n",
    "\n",
    "data_transfer_balance = {\n",
    "    'transfer_balance_id': [1, 2, 3, 4],\n",
    "    'total_amount': [100, 200, 150, 500]\n",
    "}\n",
    "\n",
    "df_transfers = pd.DataFrame(data_transfers)\n",
    "df_transfer_balance = pd.DataFrame(data_transfer_balance)\n",
    "\n",
    "merged_df = pd.merge(df_transfers, df_transfer_balance, on='transfer_balance_id', how='left')\n",
    "\n",
    "context = gx.get_context()\n",
    "\n",
    "data_source = context.data_sources.add_pandas(\"example\")\n",
    "merge_data_asset = data_source.add_dataframe_asset(name=\"merge\")\n",
    "merge_batch = merge_data_asset.add_batch_definition_whole_dataframe(\"merge\").get_batch(batch_parameters={\"dataframe\": merged_df})\n",
    "\n",
    "expectation = gx.expectations.core.ExpectColumnPairValuesToBeEqual(\n",
    "    column_A=\"amount\",\n",
    "    column_B=\"total_amount\"\n",
    ")\n",
    "\n",
    "merge_batch.validate(expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9cb3dd-34ab-4074-a7f6-6639a0a005fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a33a674-e215-49bc-9522-1376a2c8f6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/24 15:32:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-------------+------------+\n",
      "|transfer_balance_id|amount|transfer_date|total_amount|\n",
      "+-------------------+------+-------------+------------+\n",
      "|                  1|   100|   2023-01-01|         100|\n",
      "|                  3|   150|   2023-01-03|         150|\n",
      "+-------------------+------+-------------+------------+\n",
      "\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": false,\n",
       "  \"expectation_config\": {\n",
       "    \"type\": \"unexpected_rows_expectation\",\n",
       "    \"kwargs\": {\n",
       "      \"batch_id\": \"example-transfers\",\n",
       "      \"unexpected_rows_query\": \"\\n    SELECT *\\n    FROM transfers_table\\n    WHERE amount < 200\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  \"result\": {\n",
       "    \"observed_value\": 2,\n",
       "    \"details\": {\n",
       "      \"unexpected_rows\": [\n",
       "        {\n",
       "          \"transfer_balance_id\": 1,\n",
       "          \"amount\": 100,\n",
       "          \"transfer_date\": \"2023-01-01\",\n",
       "          \"total_amount\": 100\n",
       "        },\n",
       "        {\n",
       "          \"transfer_balance_id\": 3,\n",
       "          \"amount\": 150,\n",
       "          \"transfer_date\": \"2023-01-03\",\n",
       "          \"total_amount\": 150\n",
       "        }\n",
       "      ]\n",
       "    }\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FUNZIONANTE SQL (senza {batch}) + JOIN DATAFRAME SPARK + VIEW SPARK\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrames Example\").getOrCreate()\n",
    "\n",
    "data_transfers = [\n",
    "    (1, 100, '2023-01-01'),\n",
    "    (2, 200, '2023-01-02'),\n",
    "    (3, 150, '2023-01-03'),\n",
    "    (4, 500, '2023-01-04')\n",
    "]\n",
    "\n",
    "data_transfer_balance = [\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 150),\n",
    "    (4, 500)\n",
    "]\n",
    "\n",
    "df_transfers = spark.createDataFrame(data_transfers, [\"transfer_balance_id\", \"amount\", \"transfer_date\"])\n",
    "df_transfer_balance = spark.createDataFrame(data_transfer_balance, [\"transfer_balance_id\", \"total_amount\"])\n",
    "\n",
    "context = gx.get_context()\n",
    "\n",
    "data_source = context.data_sources.add_spark(\"example\")\n",
    "transfers_data_asset = data_source.add_dataframe_asset(name=\"transfers\")\n",
    "transfer_balance_data_asset = data_source.add_dataframe_asset(name=\"transfer_balance\")\n",
    "\n",
    "batch_transfers = transfers_data_asset.add_batch_definition_whole_dataframe(\"transfers\").get_batch(batch_parameters={\"dataframe\": df_transfers})\n",
    "batch_transfer_balance = transfer_balance_data_asset.add_batch_definition_whole_dataframe(\"transfer_balance\").get_batch(batch_parameters={\"dataframe\": df_transfer_balance})\n",
    "\n",
    "merged_df = df_transfers.join(df_transfer_balance, on=\"transfer_balance_id\", how=\"left\")\n",
    "\n",
    "merged_df.createOrReplaceTempView(\"transfers_table\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM transfers_table\n",
    "    WHERE amount < 200\n",
    "\"\"\"\n",
    "\n",
    "query_result = spark.sql(query)\n",
    "\n",
    "query_result.show()\n",
    "\n",
    "query_expectation = gx.expectations.UnexpectedRowsExpectation(\n",
    "    unexpected_rows_query=query\n",
    ")\n",
    "\n",
    "batch_transfers.validate(query_expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d348da10-ba4b-44f2-ab0b-900942f953de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 2/2 [00:00<00:00,  2.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": false,\n",
       "  \"expectation_config\": {\n",
       "    \"type\": \"unexpected_rows_expectation\",\n",
       "    \"kwargs\": {\n",
       "      \"batch_id\": \"example-transfers\",\n",
       "      \"unexpected_rows_query\": \"\\n    SELECT *\\n    FROM {batch}\\n    WHERE amount < 200\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  \"result\": {\n",
       "    \"observed_value\": 2,\n",
       "    \"details\": {\n",
       "      \"unexpected_rows\": [\n",
       "        {\n",
       "          \"transfer_balance_id\": 1,\n",
       "          \"amount\": 100,\n",
       "          \"transfer_date\": \"2023-01-01\"\n",
       "        },\n",
       "        {\n",
       "          \"transfer_balance_id\": 3,\n",
       "          \"amount\": 150,\n",
       "          \"transfer_date\": \"2023-01-03\"\n",
       "        }\n",
       "      ]\n",
       "    }\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FUNZIONANTE SQL (con {batch}) + senza JOIN DATAFRAME SPARK e senza VIEW SPARK\n",
    "spark = SparkSession.builder.appName(\"DataFrames Example\").getOrCreate()\n",
    "\n",
    "data_transfers = [\n",
    "    (1, 100, '2023-01-01'),\n",
    "    (2, 200, '2023-01-02'),\n",
    "    (3, 150, '2023-01-03'),\n",
    "    (4, 500, '2023-01-04')\n",
    "]\n",
    "\n",
    "data_transfer_balance = [\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 150),\n",
    "    (4, 500)\n",
    "]\n",
    "\n",
    "df_transfers = spark.createDataFrame(data_transfers, [\"transfer_balance_id\", \"amount\", \"transfer_date\"])\n",
    "df_transfer_balance = spark.createDataFrame(data_transfer_balance, [\"transfer_balance_id\", \"total_amount\"])\n",
    "\n",
    "context = gx.get_context()\n",
    "\n",
    "data_source = context.data_sources.add_spark(\"example\")\n",
    "transfers_data_asset = data_source.add_dataframe_asset(name=\"transfers\")\n",
    "transfer_balance_data_asset = data_source.add_dataframe_asset(name=\"transfer_balance\")\n",
    "\n",
    "batch_transfers = transfers_data_asset.add_batch_definition_whole_dataframe(\"transfers\").get_batch(batch_parameters={\"dataframe\": df_transfers})\n",
    "batch_transfer_balance = transfer_balance_data_asset.add_batch_definition_whole_dataframe(\"transfer_balance\").get_batch(batch_parameters={\"dataframe\": df_transfer_balance})\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM {batch}\n",
    "    WHERE amount < 200\n",
    "\"\"\"\n",
    "\n",
    "query_expectation = gx.expectations.UnexpectedRowsExpectation(\n",
    "    unexpected_rows_query=query\n",
    ")\n",
    "\n",
    "batch_transfers.validate(query_expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be651ed4-db59-48c4-ad83-de1916a8a2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|transfer_balance_id|\n",
      "+-------------------+\n",
      "|                  4|\n",
      "+-------------------+\n",
      "\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n",
      "unexpected_rows_query should contain the {batch} parameter. Otherwise data outside the configured batch will be queried.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": false,\n",
       "  \"expectation_config\": {\n",
       "    \"type\": \"unexpected_rows_expectation\",\n",
       "    \"kwargs\": {\n",
       "      \"batch_id\": \"example-transfers\",\n",
       "      \"unexpected_rows_query\": \"\\n    SELECT t.transfer_balance_id\\n    FROM transfers t\\n    LEFT JOIN transfer_balance b ON t.transfer_balance_id = b.transfer_balance_id\\n    WHERE b.transfer_balance_id IS NULL\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  \"result\": {\n",
       "    \"observed_value\": 1,\n",
       "    \"details\": {\n",
       "      \"unexpected_rows\": [\n",
       "        {\n",
       "          \"transfer_balance_id\": 4\n",
       "        }\n",
       "      ]\n",
       "    }\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FUNZIONANTE: DATAFRAME SPARK + QUERY senza {batch} ma con VIEW spark\n",
    "spark = SparkSession.builder.appName(\"DataFrames Example\").getOrCreate()\n",
    "\n",
    "data_transfers = [\n",
    "    (1, 100, '2023-01-01'),\n",
    "    (2, 200, '2023-01-02'),\n",
    "    (3, 150, '2023-01-03'),\n",
    "    (4, 500, '2023-01-04')\n",
    "]\n",
    "\n",
    "data_transfer_balance = [\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 150)#,\n",
    "    #(4, 500)\n",
    "]\n",
    "\n",
    "df_transfers = spark.createDataFrame(data_transfers, [\"transfer_balance_id\", \"amount\", \"transfer_date\"])\n",
    "df_transfer_balance = spark.createDataFrame(data_transfer_balance, [\"transfer_balance_id\", \"total_amount\"])\n",
    "\n",
    "context = gx.get_context()\n",
    "\n",
    "data_source = context.data_sources.add_spark(\"example\")\n",
    "transfers_data_asset = data_source.add_dataframe_asset(name=\"transfers\")\n",
    "transfer_balance_data_asset = data_source.add_dataframe_asset(name=\"transfer_balance\")\n",
    "\n",
    "batch_transfers = transfers_data_asset.add_batch_definition_whole_dataframe(\"transfers\").get_batch(batch_parameters={\"dataframe\": df_transfers})\n",
    "batch_transfer_balance = transfer_balance_data_asset.add_batch_definition_whole_dataframe(\"transfer_balance\").get_batch(batch_parameters={\"dataframe\": df_transfer_balance})\n",
    "\n",
    "df_transfers.createOrReplaceTempView(\"transfers\")\n",
    "df_transfer_balance.createOrReplaceTempView(\"transfer_balance\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT t.transfer_balance_id\n",
    "    FROM transfers t\n",
    "    LEFT JOIN transfer_balance b ON t.transfer_balance_id = b.transfer_balance_id\n",
    "    WHERE b.transfer_balance_id IS NULL\n",
    "\"\"\"\n",
    "\n",
    "query_result = spark.sql(query)\n",
    "\n",
    "query_result.show()\n",
    "\n",
    "query_expectation = gx.expectations.UnexpectedRowsExpectation(\n",
    "    unexpected_rows_query=query\n",
    ")\n",
    "\n",
    "batch_transfers.validate(query_expectation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807a9c7-5cc2-4112-b59c-b4be55d34218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONANTE: DATAFRAME SPARK + QUERY con {batch}\n",
    "spark = SparkSession.builder.appName(\"DataFrames Example\").getOrCreate()\n",
    "\n",
    "data_transfers = [\n",
    "    (1, 100, '2023-01-01'),\n",
    "    (2, 200, '2023-01-02'),\n",
    "    (3, 150, '2023-01-03'),\n",
    "    (4, 500, '2023-01-04')\n",
    "]\n",
    "\n",
    "data_transfer_balance = [\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 150),\n",
    "    (4, 500)\n",
    "]\n",
    "\n",
    "df_transfers = spark.createDataFrame(data_transfers, [\"transfer_balance_id\", \"amount\", \"transfer_date\"])\n",
    "df_transfer_balance = spark.createDataFrame(data_transfer_balance, [\"transfer_balance_id\", \"total_amount\"])\n",
    "\n",
    "context = gx.get_context()\n",
    "\n",
    "data_source = context.data_sources.add_spark(\"example\")\n",
    "transfers_data_asset = data_source.add_dataframe_asset(name=\"transfers\")\n",
    "transfer_balance_data_asset = data_source.add_dataframe_asset(name=\"transfer_balance\")\n",
    "\n",
    "batch_transfers = transfers_data_asset.add_batch_definition_whole_dataframe(\"transfers\").get_batch(batch_parameters={\"dataframe\": df_transfers})\n",
    "batch_transfer_balance = transfer_balance_data_asset.add_batch_definition_whole_dataframe(\"transfer_balance\").get_batch(batch_parameters={\"dataframe\": df_transfer_balance})\n",
    "\n",
    "df_transfers.createOrReplaceTempView(\"transfers\")\n",
    "df_transfer_balance.createOrReplaceTempView(\"transfer_balance\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT t.transfer_balance_id\n",
    "    FROM {batch} t\n",
    "    LEFT JOIN transfer_balance b ON t.transfer_balance_id = b.transfer_balance_id\n",
    "    WHERE b.transfer_balance_id IS NULL\n",
    "\"\"\"\n",
    "\n",
    "query_expectation = gx.expectations.UnexpectedRowsExpectation(\n",
    "    unexpected_rows_query=query\n",
    ")\n",
    "\n",
    "batch_transfers.validate(query_expectation, result_format=\"COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86663b56-aee7-40ca-ac2a-f29a776ee0de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
